# Neural-Network-Backprop-Numpy  
Implemented a neural network from scratch using the **Numpy** library to understand the core concepts of neural network architecture, training, and evaluation.

## ğŸš€ Project Overview  
This project demonstrates the construction of a neural network from the ground up, focusing on forward propagation, backpropagation, and gradient descent optimization. It includes multiple activation functions and compares the performance of the custom neural network with **scikit-learn's** neural network implementation.  

## ğŸ› ï¸ Features  
- **Activation Functions Implemented**:
  - ReLU
  - Linear
  - Sigmoid
  - TanH
  - Softmax
- **Loss Function**: Cross Entropy Loss  
- **Optimization**: Stochastic Gradient Descent (SGD)  

### âš™ï¸ Hyperparameters:
- Epochs
- Layer sizes
- Activation functions
- Batch size
- Learning rate  

### ğŸ§ª Dataset Used:  
- **MNIST**: A widely used dataset of handwritten digits, used for training and testing the neural network.  

### ğŸ” Performance Comparison:  
- **Custom Neural Network vs. scikit-learnâ€™s MLPClassifier**:  
  Losses and testing accuracies are compared to evaluate the performance of both models.  

## ğŸ“Š Results  
- Achieved comparable performance with **scikit-learn**'s neural network.
- Visualized the training loss and accuracy over epochs to understand model behavior.
