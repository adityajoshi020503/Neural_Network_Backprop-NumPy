# Neural-Network-Backprop-Numpy  
Implemented a neural network from scratch using the **Numpy** library to understand the core concepts of neural network architecture, training, and evaluation.

## 🚀 Project Overview  
This project demonstrates the construction of a neural network from the ground up, focusing on forward propagation, backpropagation, and gradient descent optimization. It includes multiple activation functions and compares the performance of the custom neural network with **scikit-learn's** neural network implementation.  

## 🛠️ Features  
- **Activation Functions Implemented**:
  - ReLU
  - Linear
  - Sigmoid
  - TanH
  - Softmax
- **Loss Function**: Cross Entropy Loss  
- **Optimization**: Stochastic Gradient Descent (SGD)  

### ⚙️ Hyperparameters:
- Epochs
- Layer sizes
- Activation functions
- Batch size
- Learning rate  

### 🧪 Dataset Used:  
- **MNIST**: A widely used dataset of handwritten digits, used for training and testing the neural network.  

### 🔍 Performance Comparison:  
- **Custom Neural Network vs. scikit-learn’s MLPClassifier**:  
  Losses and testing accuracies are compared to evaluate the performance of both models.  

## 📊 Results  
- Achieved comparable performance with **scikit-learn**'s neural network.
- Visualized the training loss and accuracy over epochs to understand model behavior.
